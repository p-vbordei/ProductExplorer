{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lark\n",
      "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install lark\n",
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "# Read the ASIN values from the CSV file\n",
    "asin_list_path = '/Users/vladbordei/Documents/Development/ProductExplorer/data/external/asin_list.csv'\n",
    "#asin_list_path = './data/external/asin_list.csv'\n",
    "asin_list = pd.read_csv(asin_list_path)['asin'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('/Users/vladbordei/Documents/Development/ProductExplorer/data/processed/reviews_export.csv')\n",
    "reviews = reviews[reviews['asin'].isin(asin_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeding and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Database: Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column='review')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x29c2b1960>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## VERSIUNEA PERSISTENTA #############\n",
    "# https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/chroma_self_query.html\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = '/Users/vladbordei/Documents/Development/ProductExplorer/data/vectorstores/chroma/db'\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x285fd3f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### VERSIUNEA TEMPORARA ##############\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you save to file and stop the database\n",
    "\n",
    "vectorstore.persist()\n",
    "vectorstore = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the persisted database from disk, and use it as normal. \n",
    "\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what I would like to see improved is the quality of the product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This product is great however can get annoying after time', metadata={'id': 3, 'rating': 5, 'review_summary': 'Product is great but can get annoying over time.', 'product_facts': 'Unknown', 'positive_sentiment': 0.50126034, 'negative_sentiment': 0.064283654, 'improvements_expected': 'Unknown', 'issues_identified': 'Product can get annoying over time', 'how_product_is_used': 'Unknown', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Neutral', 'anger': 'No', 'anger_reason': nan, 'delight': 'No', 'delight_reason': nan, 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'Super satisfying/ can get annoying', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'}),\n",
       " Document(page_content='Only complaint I have is I wish the pen was connected Car use becomes a hassle if the pen gets dropped', metadata={'id': 82, 'rating': 5, 'review_summary': 'Pen connectivity issue for car use', 'product_facts': 'Pen not connected to device', 'positive_sentiment': 0.39164537, 'negative_sentiment': 0.144063, 'improvements_expected': 'Pen connectivity to device', 'issues_identified': 'Pen gets dropped during car use', 'how_product_is_used': 'Car use', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Neutral', 'anger': 'No', 'anger_reason': nan, 'delight': 'No', 'delight_reason': nan, 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'My kids love this toy', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'}),\n",
       " Document(page_content='I am loving the product My son is 11 NonVerbal Autistic and is loving this as well The picture is how it is looking after taking it on a car ride Im not quite sure why its doing that The holes have the balls in them they just wouldnt come up like they are supposed to I will troubleshoot it to find out Otherwise its a Great Product', metadata={'id': 26, 'rating': 4, 'review_summary': 'Great product for nonverbal autistic children, but some troubleshooting required.', 'product_facts': 'Unknown', 'positive_sentiment': 0.726388, 'negative_sentiment': 0.023107188, 'improvements_expected': 'Unknown', 'issues_identified': 'Balls not coming up as they are supposed to', 'how_product_is_used': 'Unknown', 'media': '[\"https://m.media-amazon.com/images/I/71OPaLOsArL._SL1600_.jpg\"]', 'where_product_is_used': 'In a car', 'sentiment': 'Positive', 'anger': 'No', 'anger_reason': nan, 'delight': 'Yes', 'delight_reason': 'Loving the product, son enjoys it too', 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Parent of an 11-year-old nonverbal autistic child', 'title': 'Love the product', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'}),\n",
       " Document(page_content='I paid for a new product but I received something that seems damaged  used', metadata={'id': 279, 'rating': 1, 'review_summary': 'Received damaged/used product instead of new', 'product_facts': 'Unknown', 'positive_sentiment': 0.01717107, 'negative_sentiment': 0.8208156, 'improvements_expected': 'Replacement of damaged/used product with a new one', 'issues_identified': 'Received damaged/used product instead of new', 'how_product_is_used': 'Unknown', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Anger', 'anger': 'Yes', 'anger_reason': 'Received damaged/used product instead of new', 'delight': 'No', 'delight_reason': nan, 'disappointment': 'Yes', 'disappointment_reason': 'Received damaged/used product instead of new', 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'Not good', 'asin_variant': 'B083CX6B27', 'asin': 'B07Q899BPB'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(query)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='This product is great however can get annoying after time', metadata={'id': 3, 'rating': 5, 'review_summary': 'Product is great but can get annoying over time.', 'product_facts': 'Unknown', 'positive_sentiment': 0.50126034, 'negative_sentiment': 0.064283654, 'improvements_expected': 'Unknown', 'issues_identified': 'Product can get annoying over time', 'how_product_is_used': 'Unknown', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Neutral', 'anger': 'No', 'anger_reason': nan, 'delight': 'No', 'delight_reason': nan, 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'Super satisfying/ can get annoying', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'}),\n",
       " 1.1258236169815063)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='This product is great however can get annoying after time', metadata={'id': 3, 'rating': 5, 'review_summary': 'Product is great but can get annoying over time.', 'product_facts': 'Unknown', 'positive_sentiment': 0.50126034, 'negative_sentiment': 0.064283654, 'improvements_expected': 'Unknown', 'issues_identified': 'Product can get annoying over time', 'how_product_is_used': 'Unknown', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Neutral', 'anger': 'No', 'anger_reason': nan, 'delight': 'No', 'delight_reason': nan, 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'Super satisfying/ can get annoying', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'}),\n",
       " 1.1258236169815063)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">langchain.llms</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> OpenAI                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">langchain.chains</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ConversationalRetrievalChain                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>), vectorstore.as_retriev     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mlangchain\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mllms\u001b[0m \u001b[94mimport\u001b[0m OpenAI                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mlangchain\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mchains\u001b[0m \u001b[94mimport\u001b[0m ConversationalRetrievalChain                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=\u001b[94m0\u001b[0m), vectorstore.as_retriev     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'memory'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the best feature of this product\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best feature of this product is that there is no mess or cleanup.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a different model for condensing the question\n",
    "This chain has two steps:\n",
    "- First, it condenses the current question and the chat history into a standalone question. This is neccessary to create a standanlone vector to use for retrieval. \n",
    "- After that, it does retrieval and then answers the question using retrieval augmented generation with a separate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),   # model='gpt-4'),\n",
    "    vectorstore.as_retriever(),\n",
    "    condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are Heat-bath random walks with Markov base?\",\n",
    "    \"What is the ImageBind model?\",\n",
    "    \"How does Compositional Reasoning with Large Language Models works?\",   \n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What is the best feature of this product\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Why is that?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All toghtether now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationalRetrievalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(\n\u001b[1;32m      2\u001b[0m     ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m),   \u001b[39m# model='gpt-4'),\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     vectorstore\u001b[39m.\u001b[39mas_retriever(),\n\u001b[1;32m      4\u001b[0m     condense_question_llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     memory \u001b[39m=\u001b[39m memory\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConversationalRetrievalChain' is not defined"
     ]
    }
   ],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),   # model='gpt-4'),\n",
    "    vectorstore.as_retriever(),\n",
    "    condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### RETREIVAL ##########\n",
    "### VESPA ?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreival Augmented Generation (RAG)\n",
    "Improving Zero-Shot Ranking with Vespa Hybrid Search - Information Retreival Evaluation\n",
    "\n",
    "\n",
    "Information Retreival\n",
    "vespa.ai\n",
    "\n",
    "\n",
    "\n",
    "Alte metode:\n",
    "multi-dense\n",
    "\n",
    "\n",
    "BM25 ( elasticsearch) e mai bun decat simmilarity search acum ( prima impresie)\n",
    "\n",
    "\n",
    "\n",
    "Synthetic Query Generation at Spotify for improving retreival\n",
    "https://www.youtube.com/watch?v=VrL7AbrY438\n",
    "\n",
    "\n",
    "\"For every topic generate five questions that could help define / increase precision\"\n",
    "\n",
    "\n",
    "colBERT < -- retreival general foarte bun  >\n",
    "DSP Retreival < -- se poate specializa\n",
    "\n",
    "\n",
    "Scalable Nearest Neighbor Search\n",
    "PLAID Retreival Engine\n",
    "\n",
    "\n",
    "github.com/standford-futuredata/ColBERT\n",
    "\n",
    "Vespa < - hosted>\n",
    "\n",
    "\n",
    "ColBERT-QA\n",
    "Hindsight\n",
    "\n",
    "DSP Programming Model\n",
    "for highly specialized retreivers, which may be used by agents\n",
    "\n",
    "\n",
    "Run the full cross encoder model\n",
    "https://pyvespa.readthedocs.io/en/latest/deploy-vespa-cloud.html\n",
    "https://docs.vespa.ai/en/vespa-quick-start.html\n",
    "\n",
    "\n",
    "\n",
    "IN DOCUMENT CROSS-ENCODER\n",
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/in_document_search_crossencoder.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "https://www.youtube.com/watch?v=VrL7AbrY438\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/contextual-compression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"What did the president say about Ketanji Jackson Brown\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pxp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
