{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate\n",
    "\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb\n",
    "\n",
    "Docker Compose for Weaviate + OpenAI\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/docker-compose.yml\n",
    "openai-cookbook/docker-compose.yml at main · openai/openai-cookbook · GitHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "# Create an SQLAlchemy engine to connect to the database\n",
    "engine = create_engine('postgresql://postgres:mysecretpassword@localhost/postgres')\n",
    "\n",
    "# Read the ASIN values from the CSV file\n",
    "asin_list_path = '/Users/vladbordei/Documents/Development/ProductExplorer/data/external/asin_list.csv'\n",
    "#asin_list_path = './data/external/asin_list.csv'\n",
    "asin_list = pd.read_csv(asin_list_path)['asin'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weaviate\n",
    "# Connect to your Weaviate instance\n",
    "client = weaviate.Client(\n",
    "    # url=\"https://your-wcs-instance-name.weaviate.network/\",\n",
    "    url=\"http://localhost:8089/\",\n",
    "    # auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"<YOUR-WEAVIATE-API-KEY>\"), # comment out this line if you are not using authentication for your Weaviate instance (i.e. for locally deployed instances)\n",
    "    additional_headers={\n",
    "        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check if your instance is live and ready\n",
    "# This should return `True`\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2 - configure Weaviate Batch, with\n",
    "# - starting batch size of 100\n",
    "# - dynamically increase/decrease based on performance\n",
    "# - add timeout retries if something goes wrong\n",
    "\n",
    "client.batch.configure(\n",
    "    batch_size=100, \n",
    "    dynamic=True,\n",
    "    timeout_retries=3,\n",
    "#   callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3 - import data\n",
    "\n",
    "print(\"Importing Data\")\n",
    "\n",
    "counter=0\n",
    "\n",
    "with client.batch as batch:\n",
    "    for line in dataset:\n",
    "        if (counter %10 == 0):\n",
    "            print(f\"Import {counter} / {len(dataset)} \")\n",
    "\n",
    "        properties = {\n",
    "            \"title\": line[\"title\"],\n",
    "            \"content\": line[\"text\"],\n",
    "            \"url\": line[\"url\"]\n",
    "        }\n",
    "        \n",
    "        batch.add_data_object(properties, \"Article\")\n",
    "        counter = counter+1\n",
    "\n",
    "print(\"Importing Articles complete\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear up the schema, so that we can recreate it\n",
    "client.schema.delete_all()\n",
    "client.schema.get()\n",
    "\n",
    "# Define the Schema object to use `text-embedding-ada-002` on `title` and `content`, but skip it for `url`\n",
    "article_schema = {\n",
    "    \"class\": \"Article\",\n",
    "    \"description\": \"A collection of articles\",\n",
    "    \"vectorizer\": \"text2vec-openai\",\n",
    "    \"moduleConfig\": {\n",
    "        \"text2vec-openai\": {\n",
    "          \"model\": \"ada\",\n",
    "          \"modelVersion\": \"002\",\n",
    "          \"type\": \"text\"\n",
    "        }, \n",
    "        \"qna-openai\": {\n",
    "          \"model\": \"text-davinci-002\",\n",
    "          \"maxTokens\": 16,\n",
    "          \"temperature\": 0.0,\n",
    "          \"topP\": 1,\n",
    "          \"frequencyPenalty\": 0.0,conda install -c conda-forge sentence-transformers\n",
    "          \"presencePenalty\": 0.0\n",
    "        }\n",
    "    },\n",
    "    \"properties\": [{\n",
    "        \"name\": \"title\",\n",
    "        \"description\": \"Title of the article\",\n",
    "        \"dataType\": [\"string\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"content\",\n",
    "        \"description\": \"Contents of the article\",\n",
    "        \"dataType\": [\"text\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"url\",\n",
    "        \"description\": \"URL to the article\",\n",
    "        \"dataType\": [\"string\"],\n",
    "        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n",
    "    }]\n",
    "}\n",
    "\n",
    "# add the Article schema\n",
    "client.schema.create_class(article_schema)\n",
    "\n",
    "# get the schema to make sure it worked\n",
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'postgresql://postgres:mysecretpassword@localhost/postgres'\n",
    "load_dotenv()\n",
    "engine = create_engine(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "embedder = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_categories(engine=engine, data_table='weighted_trait_graph'):\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT type FROM {data_table};\n",
    "        \"\"\"\n",
    "    type_list = pd.read_sql_query(query, engine)\n",
    "    return type_list    \n",
    "\n",
    "types = get_type_categories(engine=engine, data_table='weighted_trait_graph')\n",
    "types_list = types['type'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_data(type, engine=engine, data_table='weighted_trait_graph'):\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT data_label FROM {data_table} WHERE type = '{type}';\n",
    "        \"\"\"\n",
    "    selected_data = pd.read_sql_query(query, engine)\n",
    "    return selected_data\n",
    "\n",
    "type = types_list[0]\n",
    "df = get_type_data(type = type, data_table = 'weighted_trait_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentences and store the embeddings in the 'embeddings' column\n",
    "df['embedding'] = df['data_label'].apply(lambda x: embedder.encode(x))\n",
    "\n",
    "# Normalize the embeddings to unit length\n",
    "df['embedding'] = df['embedding'].apply(lambda x: x / np.linalg.norm(x))\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(np.array)  # convert string to numpy array\n",
    "matrix = np.vstack(df.embedding.values)\n",
    "\n",
    "# Fit clusters\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1)\n",
    "cluster_model = clustering.fit(matrix)\n",
    "\n",
    "cluster_assignment = cluster_model.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  6\n",
      "['improved magnet strength and quality', 'improved magnet strength', 'improved magnet strength and functionality', 'improved quality control for magnets', 'improved magnet strength, quality, and functionality', 'improved noise level and magnet strength', 'improved magnet functionality', 'improved magnet pushability']\n",
      "\n",
      "Cluster  20\n",
      "['added string attachment for pen to prevent loss', 'added string attachment for pens to prevent loss', 'added string attachment for pen and more color options', 'added string attachment for pen']\n",
      "\n",
      "Cluster  3\n",
      "['improved pen security to the board', 'improved secure attachment of pen to board', 'improved secure attachment for pen', 'improved secure attachment for pen and noise level', 'improved pen storage design']\n",
      "\n",
      "Cluster  5\n",
      "['added more engaging features for adults', 'added more hole pattern options for adults']\n",
      "\n",
      "Cluster  14\n",
      "['improved noise level for quieter operation', 'reduced noise levels', 'improved noise level with warning on packaging', 'improved noise level for quieter operation during plane travel']\n",
      "\n",
      "Cluster  2\n",
      "['improved quality control with attached pen', 'improved stylus attachment mechanism', 'improved pen attachment design', 'improved pen attachment to the board', 'improved pen attachment to the product', 'improved pen attachment to the product with connection', 'improved pen attachment design with less focus on profit', 'improved pen attachment mechanism', 'improved durability, quality control, and pen attachment fix']\n",
      "\n",
      "Cluster  10\n",
      "['improved ball containment', 'improved ball raising mechanism']\n",
      "\n",
      "Cluster  7\n",
      "['added replacement or refund option', 'added replacement of damaged/used product with a new one']\n",
      "\n",
      "Cluster  24\n",
      "['added lanyard attachment for pen', 'added pen attachment to notepad']\n",
      "\n",
      "Cluster  13\n",
      "['improved precision and perfect pattern design, better overall design and quality beads', 'improved precision and perfect pattern design']\n",
      "\n",
      "Cluster  17\n",
      "['added larger size options']\n",
      "\n",
      "Cluster  18\n",
      "['improved durability, rust prevention design, and quality control', 'improved durability and rust prevention design']\n",
      "\n",
      "Cluster  25\n",
      "['improved noise level and ball mechanism', 'improved noise level and magnetism to keep balls on board']\n",
      "\n",
      "Cluster  15\n",
      "['improved magnet functionality with attached pen', 'improved pen and magnet sturdiness and better design to prevent beads from coming up']\n",
      "\n",
      "Cluster  1\n",
      "['fixed issue with beads getting stuck', 'fixed issue with balls not dropping down']\n",
      "\n",
      "Cluster  9\n",
      "['improved safety design and quality control', 'improved durability and quality control']\n",
      "\n",
      "Cluster  23\n",
      "['improved stylus and drawing experience with individual bead response']\n",
      "\n",
      "Cluster  12\n",
      "['added pen connectivity to device', 'added stylus and pen attachment to the board with connection', 'added stylus attachment to the board', 'added pen connectivity to device and improved pen attachment to the device']\n",
      "\n",
      "Cluster  11\n",
      "['improved packaging for better shipping protection', 'clear communication about item condition and improved packaging']\n",
      "\n",
      "Cluster  19\n",
      "['added replacement balls or boards']\n",
      "\n",
      "Cluster  16\n",
      "['no specific improvements mentioned']\n",
      "\n",
      "Cluster  8\n",
      "['improved magnet erasability and sleeker design', 'improved magnet erasing feature and sleeker design']\n",
      "\n",
      "Cluster  4\n",
      "['improved tracing on letter board side', 'improved drawing capabilities']\n",
      "\n",
      "Cluster  21\n",
      "['improved noise level and added engaging features for kids']\n",
      "\n",
      "Cluster  22\n",
      "['reduced clicking noise']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assign the cluster assignments to the DataFrame\n",
    "df['cluster_assignment'] = cluster_assignment\n",
    "\n",
    "# Create a dictionary to store the clustered sentences\n",
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "    clustered_sentences[cluster_id].append(df['data_label'].iloc[sentence_id])\n",
    "\n",
    "# Print the clusters\n",
    "for i, cluster in clustered_sentences.items():\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
