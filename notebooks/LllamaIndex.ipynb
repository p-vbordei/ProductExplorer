{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, VectorStoreIndex, ServiceContext\n",
    "from langchain import OpenAI\n",
    "\n",
    "...\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n",
    "\n",
    "# configure service context\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Philosophy: Progressive Disclosure of Complexity\n",
    "Progressive disclosure of complexity is a design philosophy that aims to strike a balance between the needs of beginners and experts. The idea is that you should give users the simplest and most straightforward interface or experience possible when they first encounter a system or product, but then gradually reveal more complexity and advanced features as users become more familiar with the system. This can help prevent users from feeling overwhelmed or intimidated by a system that seems too complex, while still giving experienced users the tools they need to accomplish advanced tasks.\n",
    "\n",
    "https://gpt-index.readthedocs.io/en/latest/guides/primer/query_interface.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying an index or a graph involves a three main components:\n",
    "\n",
    "Retrievers: A retriever class retrieves a set of Nodes from an index given a query.\n",
    "\n",
    "Response Synthesizer: This class takes in a set of Nodes and synthesizes an answer given a query.\n",
    "\n",
    "Query Engine: This class takes in a query and returns a Response object. It can make use of Retrievers and Response Synthesizer modules under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html\n",
    "\n",
    "Right now, we support the following options:\n",
    "\n",
    "default: “create and refine” an answer by sequentially going through each retrieved Node; This makes a separate LLM call per Node. Good for more detailed answers.\n",
    "\n",
    "compact: “compact” the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, “create and refine” an answer by going through multiple prompts.\n",
    "\n",
    "tree_summarize: Given a set of Node objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes.\n",
    "\n",
    "no_text: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes. The response object is covered in more detail in Section 5.\n",
    "\n",
    "accumulate: Given a set of Node objects and the query, apply the query to each Node text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ListIndex.from_documents(documents)\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "# default\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='default')\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "# compact\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact')\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "# tree summarize\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "# no text\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='no_text')\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
