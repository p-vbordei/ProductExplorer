{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lark\n",
      "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install lark\n",
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "# Read the ASIN values from the CSV file\n",
    "asin_list_path = '/Users/vladbordei/Documents/Development/ProductExplorer/data/external/asin_list.csv'\n",
    "#asin_list_path = './data/external/asin_list.csv'\n",
    "asin_list = pd.read_csv(asin_list_path)['asin'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('/Users/vladbordei/Documents/Development/ProductExplorer/data/processed/reviews_export.csv')\n",
    "reviews = reviews[reviews['asin'].isin(asin_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'rating', 'review_summary', 'product_facts', 'positive_sentiment',\n",
       "       'negative_sentiment', 'improvements_expected', 'issues_identified',\n",
       "       'how_product_is_used', 'media', 'where_product_is_used', 'sentiment',\n",
       "       'anger', 'anger_reason', 'delight', 'delight_reason', 'disappointment',\n",
       "       'disappointment_reason', 'time', 'season', 'weather',\n",
       "       'user_description', 'title', 'review', 'asin_variant', 'asin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeding and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dataframe, column_name):\n",
    "    # Extract single values from lists\n",
    "    dataframe[column_name] = dataframe[column_name].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else [x][0] if not isinstance(x, list) else x)\n",
    "\n",
    "    # Replace missing values with NaN\n",
    "    dataframe[column_name].replace(['', 'NA', 'N/A', 'missing', 'NaN', 'unknown', 'Unknown', ['Unknown']], np.nan, inplace=True)\n",
    "\n",
    "    # Drop NaN values\n",
    "    dataframe.dropna(subset=[column_name], inplace=True)\n",
    "\n",
    "    # Replace missing values with 'Unknown'\n",
    "    dataframe[column_name].fillna(value='unknown', inplace=True)\n",
    "\n",
    "    # Drop 'Unknown' values\n",
    "    dataframe = dataframe[dataframe[column_name] != 'unknown']\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def transform_string(input_string):\n",
    "    words = input_string.split('_')  # Split the string by underscores\n",
    "    capitalized_words = [word.capitalize() for word in words]  # Capitalize each word\n",
    "    transformed_string = ' '.join(capitalized_words)  # Join the words back into a string\n",
    "    return transformed_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### IN VERSIUNEA ASTA DATELE SUNT INCARCATE COLOANA CU COLOANA #######\n",
    "\n",
    "df_index_columns = ['anger',\n",
    " 'asin',\n",
    " 'asin_variant',\n",
    " 'delight',\n",
    " 'disappointment',\n",
    " 'id',\n",
    " 'media',\n",
    " 'negative_sentiment',\n",
    " 'positive_sentiment',\n",
    " 'rating',\n",
    " 'sentiment']\n",
    "df_data_columns = ['review_summary', 'product_facts', 'improvements_expected','issues_identified', \"review\",'anger_reason',  'delight_reason', 'disappointment_reason', 'time', 'season', 'weather', 'user_description','title','where_product_is_used','how_product_is_used']\n",
    "df_index = df[df_index_columns].copy()\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "for column in df_data_columns:\n",
    "    df_data = df[[column] + df_index_columns].copy()\n",
    "    df_data = clean_data(df_data, column)\n",
    "    df_data[column] = transform_string(column) + \": \" + df_data[column].astype(str)\n",
    "    df_data['record_type'] = column\n",
    "\n",
    "    try:\n",
    "        loader = DataFrameLoader(df_data, page_content_column=column)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        print(column)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### IN VERSIUNEA ASTA DATELE SUNT INLANTUITE INTR-UN SINGUR STRING #######\n",
    "\n",
    "df_index_columns = ['anger',\n",
    " 'asin',\n",
    " 'asin_variant',\n",
    " 'delight',\n",
    " 'disappointment',\n",
    " 'id',\n",
    " 'media',\n",
    " 'negative_sentiment',\n",
    " 'positive_sentiment',\n",
    " 'rating',\n",
    " 'sentiment']\n",
    "df_data_columns = ['review_summary', 'product_facts', 'improvements_expected','issues_identified', \"review\",'anger_reason',  'delight_reason', 'disappointment_reason', 'time', 'season', 'weather', 'user_description','title','where_product_is_used','how_product_is_used']\n",
    "df_index = df[df_index_columns].copy()\n",
    "df_data = df[df_data_columns]\n",
    "data_dict = df_data.to_dict(orient='records')\n",
    "df_data_page = df_index.copy()\n",
    "df_data_page['data'] = str(data_dict)\n",
    "loader = DataFrameLoader(df_data_page, page_content_column='data')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, page_content_column='review')\n",
    "documents = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Database: Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x29c2144f0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/chroma_self_query.html\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = '/Users/vladbordei/Documents/Development/ProductExplorer/data/vectorstores/chroma/db'\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x29c2142e0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you save to file and stop the database\n",
    "\n",
    "vectorstore.persist()\n",
    "vectorstore = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the persisted database from disk, and use it as normal. \n",
    "\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hate that feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This product is great however can get annoying after time', metadata={'id': 3, 'rating': 5, 'review_summary': 'Product is great but can get annoying over time.', 'product_facts': 'Unknown', 'positive_sentiment': 0.50126034, 'negative_sentiment': 0.064283654, 'improvements_expected': 'Unknown', 'issues_identified': 'Product can get annoying over time', 'how_product_is_used': 'Unknown', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Neutral', 'anger': 'No', 'anger_reason': nan, 'delight': 'No', 'delight_reason': nan, 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'Super satisfying/ can get annoying', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "retriever.get_relevant_documents(query)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only complaint I have is I wish the pen was connected Car use becomes a hassle if the pen gets dropped\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='This product is great however can get annoying after time', metadata={'id': 3, 'rating': 5, 'review_summary': 'Product is great but can get annoying over time.', 'product_facts': 'Unknown', 'positive_sentiment': 0.50126034, 'negative_sentiment': 0.064283654, 'improvements_expected': 'Unknown', 'issues_identified': 'Product can get annoying over time', 'how_product_is_used': 'Unknown', 'media': '[]', 'where_product_is_used': 'Unknown', 'sentiment': 'Neutral', 'anger': 'No', 'anger_reason': nan, 'delight': 'No', 'delight_reason': nan, 'disappointment': 'No', 'disappointment_reason': nan, 'time': 'Unknown', 'season': 'Unknown', 'weather': 'Unknown', 'user_description': 'Unknown', 'title': 'Super satisfying/ can get annoying', 'asin_variant': 'B07XCRT49W', 'asin': 'B07X7YFZWG'}),\n",
       " 1.3280236721038818)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the best feature of this product\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best feature of this product is that there is no mess or cleanup.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a different model for condensing the question\n",
    "This chain has two steps:\n",
    "- First, it condenses the current question and the chat history into a standalone question. This is neccessary to create a standanlone vector to use for retrieval. \n",
    "- After that, it does retrieval and then answers the question using retrieval augmented generation with a separate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),   # model='gpt-4'),\n",
    "    vectorstore.as_retriever(),\n",
    "    condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What is the best feature of this product\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Why is that?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All toghtether now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationalRetrievalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(\n\u001b[1;32m      2\u001b[0m     ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m),   \u001b[39m# model='gpt-4'),\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     vectorstore\u001b[39m.\u001b[39mas_retriever(),\n\u001b[1;32m      4\u001b[0m     condense_question_llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     memory \u001b[39m=\u001b[39m memory\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConversationalRetrievalChain' is not defined"
     ]
    }
   ],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),   # model='gpt-4'),\n",
    "    vectorstore.as_retriever(),\n",
    "    condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),\n",
    "    memory = memory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pxp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
